#Advanced Text Tokenization with spaCy
import spacy

# Use spaCy for advanced tokenization and NLP tasks
nlp = spacy.load("en_core_web_sm")
text = "Advanced text tokenization with spaCy is powerful!"
doc = nlp(text)

# Access detailed token information
for token in doc:
    print(f"Token: {token.text}, Lemma: {token.lemma_}, POS: {token.pos_}")

#Handling Text Data with Transformers Library
from transformers import pipeline

# Utilize the Transformers library for various NLP tasks
classifier = pipeline('sentiment-analysis')
result = classifier('Transformers library makes NLP tasks easy and efficient!')
print(result)

#Custom Word Embeddings with Word2Vec
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize

# Train your own word embeddings using Word2Vec
corpus = "Advanced NLP models enhance language understanding."
tokens = word_tokenize(corpus)
model = Word2Vec([tokens], vector_size=10, window=3, min_count=1, workers=4)
word_vector = model.wv['NLP']
print(f"Word Vector for 'NLP': {word_vector}")

#Named Entity Recognition (NER) with spaCy
import spacy

# Perform Named Entity Recognition (NER) using spaCy
nlp = spacy.load("en_core_web_sm")
text = "Natural Language Processing helps in information extraction from unstructured text."
doc = nlp(text)

# Extract named entities
for ent in doc.ents:
    print(f"Entity: {ent.text}, Label: {ent.label_}")

#Text Generation with GPT-3
import openai

# Use the OpenAI GPT-3 API for text generation
openai.api_key = 'your_api_key'
response = openai.Completion.create(
  engine="text-davinci-003",
  prompt="Generate creative text with GPT-3: ",
  max_tokens=100
)

generated_text = response['choices'][0]['text']
print(generated_text)
