# -*- coding: utf-8 -*-
"""practice web scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10UpMQaTyjsVcgFdWoKxq_8l_qO-_LL6t
"""

# Import required libraries
import requests
from bs4 import BeautifulSoup

def extract_google_maps_data(url):
    # Send a GET request to the Google Maps URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code != 200:
        print("Failed to retrieve data. Status code:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # Initialize a dictionary to store extracted data
    data = {
        'Company Name': None,
        'Phone Number': None,
        'Website URL': None,
        'Location': None,
    }

    # Extract company name (update the selectors as needed)
    company_name = soup.find('h1')  # Adjust the selector based on the actual structure
    if company_name:
        data['Company Name'] = company_name.text.strip()

    # Extract phone number (update the selectors as needed)
    phone_number = soup.find('span', class_='phone-number')  # Adjust the selector as needed
    if phone_number:
        data['Phone Number'] = phone_number.text.strip()

    # Extract website URL (update the selectors as needed)
    website_url = soup.find('a', class_='website-link')  # Adjust the selector as needed
    if website_url and 'href' in website_url.attrs:
        data['Website URL'] = website_url['href'].strip()

    # Extract location (update the selectors as needed)
    location = soup.find('span', class_='location-address')  # Adjust the selector as needed
    if location:
        data['Location'] = location.text.strip()

    return data

# Example usage with the provided link
url = 'https://www.google.com/maps/place/Allama+Iqbal+International+Airport/@31.5180513,74.4009104,15.25z/data=!4m6!3m5!1s0x39190f7df22c3fe7:0x8dd7ffda40aa6b03!8m2!3d31.5203274!4d74.4103575!16zL20vMDIxNTFw?entry=ttu&g_ep=EgoyMDI0MTAwMi4xIKXMDSoASAFQAw%3D%3D'
extracted_data = extract_google_maps_data(url)

# Print the extracted data
if extracted_data:
    print("Extracted Data:")
    for key, value in extracted_data.items():
        print(f"{key}: {value}")
else:
    print("No data extracted.")

!apt-get update # to update ubuntu to correctly run apt install
!apt install -y chromium-chromedriver
!pip install selenium

!apt-get install -y python3-pyppeteer
!pip install pyppeteer

import asyncio
from pyppeteer import launch

async def extract_google_maps_data(url):
    # Launch a headless browser with modified options
    browser = await launch({
        'headless': True,
        'args': [
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',  # Disabling GPU can help in Colab
            '--window-size=1280,800'
        ]
    })

    page = await browser.newPage()
    await page.goto(url)

    # Wait for the content to load
    await page.waitForSelector('h1')  # Adjust the selector based on the actual structure

    data = {
        'Company Name': None,
        'Phone Number': None,
        'Website URL': None,
        'Location': None,
    }

    # Extract company name
    try:
        company_name = await page.querySelectorEval('h1', '(element) => element.textContent')
        data['Company Name'] = company_name
    except Exception:
        data['Company Name'] = "Not found"

    # Extract phone number (adjust selector as needed)
    try:
        phone_number = await page.querySelectorEval('.phone-number', '(element) => element.textContent')
        data['Phone Number'] = phone_number
    except Exception:
        data['Phone Number'] = "Not found"

    # Extract website URL (adjust selector as needed)
    try:
        website_url = await page.querySelectorEval('.website-link', '(element) => element.href')
        data['Website URL'] = website_url
    except Exception:
        data['Website URL'] = "Not found"

    # Extract location (adjust selector as needed)
    try:
        location = await page.querySelectorEval('.location-address', '(element) => element.textContent')
        data['Location'] = location
    except Exception:
        data['Location'] = "Not found"

    await browser.close()
    return data

# Example usage with the provided link
url = 'https://www.google.com/maps/place/Allama+Iqbal+International+Airport/@31.5180513,74.4009104,15.25z/data=!4m6!3m5!1s0x39190f7df22c3fe7:0x8dd7ffda40aa6b03!8m2!3d31.5203274!4d74.4103575!16zL20vMDIxNTFw?entry=ttu&g_ep=EgoyMDI0MTAwMi4xIKXMDSoASAFQAw%3D%3D'

# Create a coroutine task and run it
async def main():
    data = await extract_google_maps_data(url)
    return data

# Run the main coroutine
data = await main()

# Print the extracted data
print("Extracted Data:")
for key, value in data.items():
    print(f"{key}: {value}")

import asyncio
from pyppeteer import launch

async def extract_google_maps_data(url):
    # Launch a headless browser with modified options
    browser = await launch({
        'headless': True,
        'args': [
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',  # Disabling GPU can help in Colab
            '--window-size=1280,800'
        ]
    })

    page = await browser.newPage()
    await page.goto(url)

    # Wait for the content to load
    await page.waitFor(5000)  # wait for 5 seconds to ensure everything loads

    data = {
        'Company Name': None,
        'Phone Number': None,
        'Website URL': None,
        'Location': None,
    }

    # Extract company name
    try:
        company_name = await page.querySelectorEval('h1', '(element) => element.textContent')
        data['Company Name'] = company_name
    except Exception:
        data['Company Name'] = "Not found"

    # Extract phone number (may need to adjust selector)
    try:
        phone_number = await page.querySelectorEval('a[href^="tel:"]', '(element) => element.textContent')
        data['Phone Number'] = phone_number
    except Exception:
        data['Phone Number'] = "Not found"

    # Extract website URL (may need to adjust selector)
    try:
        website_url = await page.querySelectorEval('a[href^="http"]', '(element) => element.href')
        data['Website URL'] = website_url
    except Exception:
        data['Website URL'] = "Not found"

    # Extract location (address)
    try:
        address = await page.querySelectorEval('.section-hero-header-title', '(element) => element.textContent')
        data['Location'] = address
    except Exception:
        data['Location'] = "Not found"

    await browser.close()
    return data

# Example usage with the provided link
url = 'https://www.google.com/maps/place/Allama+Iqbal+International+Airport/@31.5180513,74.4009104,15.25z/data=!4m6!3m5!1s0x39190f7df22c3fe7:0x8dd7ffda40aa6b03!8m2!3d31.5203274!4d74.4103575!16zL20vMDIxNTFw?entry=ttu&g_ep=EgoyMDI0MTAwMi4xIKXMDSoASAFQAw%3D%3D'

# Create a coroutine task and run it
async def main():
    data = await extract_google_maps_data(url)
    return data

# Run the main coroutine in an async context
data = asyncio.run(main())

# Print the extracted data
print("Extracted Data:")
for key, value in data.items():
    print(f"{key}: {value}")

import asyncio
from pyppeteer import launch

async def extract_google_maps_data(url):
    # Launch a headless browser with modified options
    browser = await launch({
        'headless': True,
        'args': [
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',  # Disabling GPU can help in Colab
            '--window-size=1280,800'
        ]
    })

    page = await browser.newPage()
    await page.goto(url)

    # Wait for the content to load
    await page.waitFor(5000)  # wait for 5 seconds to ensure everything loads

    data = {
        'Company Name': None,
        'Phone Number': None,
        'Website URL': None,
        'Location': None,
    }

    # Extract company name
    try:
        company_name = await page.querySelectorEval('h1', '(element) => element.textContent')
        data['Company Name'] = company_name
    except Exception:
        data['Company Name'] = "Not found"

    # Extract phone number (may need to adjust selector)
    try:
        phone_number = await page.querySelectorEval('a[href^="tel:"]', '(element) => element.textContent')
        data['Phone Number'] = phone_number
    except Exception:
        data['Phone Number'] = "Not found"

    # Extract website URL (may need to adjust selector)
    try:
        website_url = await page.querySelectorEval('a[href^="http"]', '(element) => element.href')
        data['Website URL'] = website_url
    except Exception:
        data['Website URL'] = "Not found"

    # Extract location (address)
    try:
        address = await page.querySelectorEval('.section-hero-header-title', '(element) => element.textContent')
        data['Location'] = address
    except Exception:
        data['Location'] = "Not found"

    await browser.close()
    return data

# Example usage with the provided link
url = 'https://lmra.gov.bh/en/page/show/133'

# Define the main function to run the extraction
async def main():
    data = await extract_google_maps_data(url)
    return data

# Run the main coroutine using the existing event loop
loop = asyncio.get_event_loop()
data = loop.run_until_complete(main())

# Print the extracted data
print("Extracted Data:")
for key, value in data.items():
    print(f"{key}: {value}")

"""**Actual Code**"""

# @title Default title text
import asyncio
from pyppeteer import launch

async def extract_google_maps_data(url):
    # Launch a headless browser with modified options
    browser = await launch({
        'headless': True,
        'args': [
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',  # Disabling GPU can help in Colab
            '--window-size=1280,800'
        ]
    })

    page = await browser.newPage()
    await page.goto(url)

    # Wait for the content to load
    await page.waitFor(5000)  # wait for 5 seconds to ensure everything loads

    data = {
        'Company Name': None,
        'Phone Number': None,
        'Website URL': None,
        'Location': None,
    }

    # Extract company name
    try:
        company_name = await page.querySelectorEval('h1', '(element) => element.textContent')
        data['Company Name'] = company_name
    except Exception:
        data['Company Name'] = "Not found"

    # Extract phone number (may need to adjust selector)
    try:
        phone_number = await page.querySelectorEval('a[href^="tel:"]', '(element) => element.textContent')
        data['Phone Number'] = phone_number
    except Exception:
        data['Phone Number'] = "Not found"

    # Extract website URL (may need to adjust selector)
    try:
        website_url = await page.querySelectorEval('a[href^="http"]', '(element) => element.href')
        data['Website URL'] = website_url
    except Exception:
        data['Website URL'] = "Not found"

    # Extract location (address)
    try:
        address = await page.querySelectorEval('.section-hero-header-title', '(element) => element.textContent')
        data['Location'] = address
    except Exception:
        data['Location'] = "Not found"

    await browser.close()
    return data

# Example usage with the provided link
url = 'https://www.google.com/maps/place/TriVA+Global/@31.1457935,72.695379,17z/data=!3m1!4b1!4m6!3m5!1s0x39230116220bf807:0x3427b003049cf1dc!8m2!3d31.1457889!4d72.6979539!16s%2Fg%2F11v0q2_10r?entry=ttu&g_ep=EgoyMDI0MTAwMi4xIKXMDSoASAFQAw%3D%3D'
# Define an async function to run the extraction and print the results
async def main():
    data = await extract_google_maps_data(url)
    # Print the extracted data
    print("Extracted Data:")
    for key, value in data.items():
        print(f"{key}: {value}")

# Run the main coroutine directly
await main()



!pip install requests
!pip install beautifulsoup4

import requests

url = "https://www.google.com/maps/place/Allama+Iqbal+International+Airport/@31.5203319,74.4077826,17z/data=!3m1!4b1!4m6!3m5!1s0x39190f7df22c3fe7:0x8dd7ffda40aa6b03!8m2!3d31.5203274!4d74.4103575!16zL20vMDIxNTFw?entry=ttu&g_ep=EgoyMDI0MTAwMi4xIKXMDSoASAFQAw%3D%3D"
response = requests.get(url)
html_content = response.text

import requests
from bs4 import BeautifulSoup

url = "https://www.google.com/maps/place/Allama+Iqbal+International+Airport/@31.5203319,74.4077826,17z/data=!3m1!4b1!4m6!3m5!1s0x39190f7df22c3fe7:0x8dd7ffda40aa6b03!8m2!3d31.5203274!4d74.4103575!16zL20vMDIxNTFw?entry=ttu&g_ep=EgoyMDI0MTAwMi4xIKXMDSoASAFQAw%3D%3D"
response = requests.get(url)
html_content = response.text

soup = BeautifulSoup(html_content, "html.parser")

restaurants = soup.find_all("div", class_="section-result-details-container")

for restaurant in restaurants:
    name = restaurant.find("h3", class_="section-result-title").text.strip()
    address = restaurant.find("span", class_="section-result-location").text.strip()

    print("Name:", name)
    print("Address:", address)
    print("-" * 50)

from bs4 import BeautifulSoup

soup = BeautifulSoup(html_content, "html.parser")

restaurants = soup.find_all("div", class_="section-result-details-container")

for restaurant in restaurants:
    # Code for name and address extraction

    rating_element = restaurant.find("span", class_="cards-rating-score")
    rating = rating_element.text.strip() if rating_element else "N/A"

    print("Name:", name)
    print("Address:", address)
    print("Rating:", rating)
    print("-" * 50)
    print("Name:", name)
    print("Address:", address)
    print("Rating:", rating)

!apt-get update
!apt-get install -y unzip xvfb libxi6 libgconf-2-4
!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!dpkg -i google-chrome-stable_current_amd64.deb
!apt-get -f install -y
!apt-get install -y google-chrome-stable

!wget https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip
!unzip chromedriver_linux64.zip
!mv chromedriver /usr/local/bin/

# Install Python packages
!pip install selenium
!pip install chromedriver_autoinstaller
!pip install beautifulsoup4 lxml

import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

!apt-get purge google-chrome-stable



# Setup the Selenium WebDriver for Colab
def driver_define():
    options = Options()
    options.add_argument('--headless')  # Run in headless mode
    options.add_argument('--no-sandbox')  # Required for Colab
    options.add_argument('--disable-dev-shm-usage')  # Required for Colab
    options.add_argument('--disable-gpu')  # Disable GPU rendering
    options.add_argument('--remote-debugging-port=9222')
    driver = webdriver.Chrome('chromedriver', options=options)
    return driver

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

# Setup the Selenium WebDriver for Colab
def driver_define():
    options = Options()
    options.add_argument('--headless')  # Run in headless mode
    options.add_argument('--no-sandbox')  # Required for Colab
    options.add_argument('--disable-dev-shm-usage')  # Required for Colab

    service = Service('/usr/local/bin/chromedriver')  # Path to the new ChromeDriver
    driver = webdriver.Chrome(service=service, options=options)
    return driver

# Setup the Selenium WebDriver for Colab
def driver_define():
    options = Options()
    options.add_argument('--headless')  # Run in headless mode
    options.add_argument('--no-sandbox')  # Required for Colab
    options.add_argument('--disable-dev-shm-usage')  # Required for Colab
    options.add_argument('--disable-gpu')  # Disable GPU rendering
    options.add_argument('--remote-debugging-port=9222')

    # Use Service to specify the path to ChromeDriver
    service = Service('/usr/local/bin/chromedriver')  # Adjust path if needed

    driver = webdriver.Chrome(service=service, options=options)
    return driver

# Main logic to scrape data from Google Maps
def scrape_data(driver, url):
    driver.get(url)
    time.sleep(2)  # Ensure the page is fully loaded

    try:
        name = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//h1'))).text
    except Exception as e:
        name = 'N/A'
        print(f"Error extracting name: {str(e)}")

    try:
        address = driver.find_element(By.XPATH, '//button[@data-item-id="address"]').text
    except:
        address = 'N/A'

    try:
        website = driver.find_element(By.CSS_SELECTOR, 'a[aria-label^="Website:"]').get_attribute('href')
    except:
        website = 'N/A'

    try:
        phone = driver.find_element(By.CSS_SELECTOR, 'button[aria-label*="Phone:"]').text
    except:
        phone = 'N/A'

    # Display the extracted data in the required format
    print(f"\n{address}")
    print(f"\n{website}")
    print(f"\n{phone}")

    return [url, name, address, website, phone]

# Run the scraper and display output
def run_scraper():
    driver = driver_define()  # Start the driver

    # Specify the Google Maps link to scrape
    url = 'https://www.google.com/maps/place/Allama+Iqbal+International+Airport/@31.5203319,74.4077826,17z/data=!3m1!4b1!4m6!3m5!1s0x39190f7df22c3fe7:0x8dd7ffda40aa6b03!8m2!3d31.5203274!4d74.4103575!16zL20vMDIxNTFw?entry=ttu&g_ep=EgoyMDI0MTAwMi4xIKXMDSoASAFQAw%3D%3D'

    try:
        data = scrape_data(driver, url)
    except Exception as e:
        print(f"Failed to scrape {url}: {str(e)}")

    driver.quit()  # Close the browser

# To call the scraper, just run this:
run_scraper()

!pip install requests

!pip install beautifulsoup4

!pip install webdriver-manager # Install the 'webdriver-manager' package using pip

# importing necessary packages
from selenium import webdriver
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

# for holding the resultant list
element_list = []

for page in range(1, 3, 1):

    page_url = "https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=" + str(page)
    driver = webdriver.Chrome(ChromeDriverManager().install())
    driver.get(page_url)
    title = driver.find_elements(By.CLASS_NAME, "title")
    price = driver.find_elements(By.CLASS_NAME, "price")
    description = driver.find_elements(By.CLASS_NAME, "description")
    rating = driver.find_elements(By.CLASS_NAME, "ratings")

    for i in range(len(title)):
        element_list.append([title[i].text, price[i].text, description[i].text, rating[i].text])

print(element_list)

#closing the driver
driver.close()

